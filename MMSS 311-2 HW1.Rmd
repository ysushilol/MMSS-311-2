---
title: "MMSS 311-2 HW1"
author: "Yushi Liu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Regression 
## OLS
```{r}
library(dplyr)
library(ggplot2)
sick <- read.csv("sick_data.csv") 
sick$dummy <- ifelse(sick$result == "Positive", 1, 0)
lm1 <- lm(dummy~bp+temp, data=sick)
summary(lm1)
sick$predicted <- ifelse (fitted(lm1) < 0.5, "Negative", "Positive")
sick$count <- ifelse (sick$result == sick$predicted, 1, 0)
sum(sick$count)/1000
```

## Logit
```{r}
lm2 <- glm(dummy~bp+temp, data=sick, family = "binomial")
summary(lm2)
sick$predicted_logit <- ifelse (fitted(lm2) < 0.5, "Negative", "Positive")
sick$count_logit <- ifelse (sick$result == sick$predicted_logit, 1, 0)
sum(sick$count_logit)/1000
c <- (0.5 - coef(lm2)["(Intercept)"])/coef(lm2)["temp"]
d <- coef(lm2)["bp"]/coef(lm2)["temp"]

```
b) The logit regression predicts 99.2% of the cases correctly, while the linear regression predicts 96.4% correctly. Statistically, the logit model predicts more accurately than the the linear regression. 
c) For the OLS model: 
$bp = 90.95175  + 0.1319124 \space temp$.

For the logit model:
$bp = -571.009822 + 6.612235 \space temp$

d) Below is the graph with OLS line:
```{r}
a <- (0.5 - coef(lm1)["(Intercept)"])/coef(lm1)["temp"]
b <- coef(lm1)["bp"]/coef(lm1)["temp"]
ggplot(sick, aes(x = bp, y= temp))+
  geom_point(aes(color = factor(sick$result ))) +
  geom_abline(intercept = a, slope = -b, color = "black")
```
Below is the graph with logit line:
```{r}
ggplot(sick, aes(x = bp, y= temp))+
  geom_point(aes(color = factor(sick$result ))) +
  geom_abline(intercept = c, slope = -d, color = "black")
```

# Regularization/Selection
a)
```{r}
widget <- read.csv("widget_data.csv")
plot(widget$y, pch=19)
```
## Ridge Regression
b)
```{r}
library(glmnet)
library(tidyverse)
library(broom)

grid = 10^seq(2, -2, length = 100)
ridge.mod = glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 0, lambda=grid) %>% broom::tidy()

#Plot Ridge
ridge.plot = ggplot(data.frame(ridge.mod), aes(x = ridge.mod$lambda, y = ridge.mod$estimate)) + geom_point()

#CV Ridge
set.seed(1)
cv.out = cv.glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
```
The best lambda is 0.3410022. 

## Lasso
```{r}
lasso.mod = glmnet(x = as.matrix(widget[, 2:31]), y = as.matrix(widget[1]), alpha = 1, lambda=grid) %>% broom::tidy()

#Plot lasso
lasso.plot = ggplot(data.frame(lasso.mod), aes(x = lasso.mod$lambda, y = lasso.mod$estimate)) + geom_point()

#CV lasso
set.seed(2)
cv.out1 = cv.glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 1)
plot(cv.out1)
bestlam1 = cv.out1$lambda.min
bestlam1
```
The best lambda for the lasso regression is 0.2092358. 
The lasso regression gives a smaller lambda than ridge regression does. Lasso regression and ridge regression give different optimal lambdas because they impose different rules of deciding penalties. 
# Classification
```{r}
library(e1071)
pol_data = read.csv("pol_data.csv")
# Split the data
pol_data["ID"] <- rownames(pol_data)
pol_data$dummy <- ifelse(pol_data$group == "Socialcrat", 1, 0)
set.seed(20)
test <- pol_data[sample(nrow(pol_data), 100), ]
train <- pol_data[!(pol_data$ID %in% test$ID),]
```
## Naive Bayes
```{r}
naiveBayes(train$dummy ~ train$pol_margin+train$col_degree+train$house_income, train, na.action = na.pass)
NB.prediction <- predict()
```
##SVM
```{r}
dat <- data.frame(x = train[,2:4], y = as.factor(train$dummy))
svmfit <- svm(y~., data=dat, kernel="linear", cost=10,
scale=FALSE)
summary(svmfit)

#SVM tune out
set.seed(250)
tune.out = tune(svm, train$dummy ~ train$pol_margin+train$col_degree+train$house_income, data = dat, kernel = "linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)

#make prediction using the best model
#testdat = data.frame(x = test[2:4], y = as.factor(test$dummy))
testdat=data.frame(x=test[2:4])
bsvm = svm(train$group~train$pol_margin+train$col_degree+train$house_income, data = train, kernel = "linear", cost = 0.1, scale = FALSE)
ypred <- predict(bestmod, as.matrix(testdat))

table(predict = ypred, truth = testdat$y)
```



