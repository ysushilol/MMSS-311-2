---
title: "MMSS 311-2 HW1"
author: "Yushi Liu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Regression 
## OLS
```{r}
library(dplyr)
library(ggplot2)
sick <- read.csv("sick_data.csv") 
sick$dummy <- ifelse(sick$result == "Positive", 1, 0)
lm1 <- lm(dummy~bp+temp, data=sick)
summary(lm1)
sick$predicted <- ifelse (fitted(lm1) < 0.5, "Negative", "Positive")
sick$count <- ifelse (sick$result == sick$predicted, 1, 0)
sum(sick$count)/1000
```

## Logit
```{r}
lm2 <- glm(dummy~bp+temp, data=sick, family = "binomial")
summary(lm2)
sick$predicted_logit <- ifelse (fitted(lm2) < 0.5, "Negative", "Positive")
sick$count_logit <- ifelse (sick$result == sick$predicted_logit, 1, 0)
sum(sick$count_logit)/1000
c <- (0.5 - coef(lm2)["(Intercept)"])/coef(lm2)["temp"]
d <- coef(lm2)["bp"]/coef(lm2)["temp"]

```
b) The logit regression predicts 99.2% of the cases correctly, while the linear regression predicts 96.4% correctly. Statistically, the logit model predicts more accurately than the the linear regression. 
c) For the OLS model: 
$bp = 90.95175  + 0.1319124 \space temp$.

For the logit model:
$bp = -571.009822 + 6.612235 \space temp$

d) Below is the graph with OLS line:
```{r}
a <- (0.5 - coef(lm1)["(Intercept)"])/coef(lm1)["temp"]
b <- coef(lm1)["bp"]/coef(lm1)["temp"]
ggplot(sick, aes(x = bp, y= temp))+
  geom_point(aes(color = factor(sick$result ))) +
  geom_abline(intercept = a, slope = -b, color = "black")
```
Below is the graph with logit line:
```{r}
ggplot(sick, aes(x = bp, y= temp))+
  geom_point(aes(color = factor(sick$result ))) +
  geom_abline(intercept = c, slope = -d, color = "black")
```

# Regularization/Selection
a)
```{r}
widget <- read.csv("widget_data.csv")
plot(widget$y, pch=19)
```
## Ridge Regression
b)
```{r}
library(glmnet)
library(tidyverse)
library(broom)

grid = 10^seq(2, -2, length = 100)
ridge.mod = glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 0, lambda=grid) %>% broom::tidy()

#Plot Ridge
ridge.plot = ggplot(data.frame(ridge.mod), aes(x = ridge.mod$lambda, y = ridge.mod$estimate)) + geom_point()

#CV Ridge
set.seed(1)
cv.out = cv.glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
```
The best lambda is 0.3410022. 

## Lasso
```{r}
lasso.mod = glmnet(x = as.matrix(widget[, 2:31]), y = as.matrix(widget[1]), alpha = 1, lambda=grid) %>% broom::tidy()

#Plot lasso
lasso.plot = ggplot(data.frame(lasso.mod), aes(x = lasso.mod$lambda, y = lasso.mod$estimate)) + geom_point()

#CV lasso
set.seed(2)
cv.out1 = cv.glmnet(x = as.matrix(widget[,2:31]), y = as.matrix(widget[1]), alpha = 1)
plot(cv.out1)
bestlam1 = cv.out1$lambda.min
bestlam1
```
The best lambda for the lasso regression is 0.2092358. 
The lasso regression gives a smaller lambda than ridge regression does. This may imply that 
## Classification
```{r}
library(e1071)

```



